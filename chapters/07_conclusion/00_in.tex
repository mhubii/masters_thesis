\label{sec::7_co}
\section{Contributions}
Within the scope of this thesis, two main sub-objectives were achieved. The first was to implement a nonlinear model predictive control, based on the works of \cite{stein2017closed}\cite{naveau2016reactive}, while the second was to utilize neural networks to achieve a high-level control of it. Therefore, neural networks were trained by two different approaches. The first approach relied on behavioral cloning, which got mainly inspired by \cite{bojarski2016end}, where it was used for autonomous navigation of self-driving cars, while the second approach used proximal policy optimization \cite{schulman2017proximal}, a form of reinforcement learning.  Within this work, to the best knowledge of the authors, it was the first time that iCub navigated an environment while being controlled by a neural network. It was also the first time that Heicub's cameras where actually being used, and an exhaustive guide on how this was done is included in the appendix.\\\\
This work began as a spin-off from previous work, which was carried out at our group \cite{stein2017closed}, and where two main future steps were proposed. The first was to re-implement the nonlinear model predictive controller together with the inverse kinematics into an enclosed library, in order to compensate for delays that were induced by the YARP network, which had to manage these two building blocks separately. This aim was successfully met, as the presented implementation ran within a single thread of the YARP network (see figure \ref{fig::341_yarp}). Furthermore, a speed-up of $600\,\%$ (section \ref{sec::411_bm}) was achieved, when compared to the previous Python implementation of the nonlinear model predictive control from within our group, which served as a template. The second proposal of the previous work was the use of additional constraints for the inverse kinematics, as hip dislocations were observed for long trajectories. Within this work, it was then found that rotational constraints to the hip solved the hip's dislocation, which for the first time allowed Heicub to move for arbitrarily long times (section \ref{sec::412_pt}). The achievement of the two proposed future steps then enabled the implementation of a real-time control, for which a terminal user interface got created (section \ref{sec::B31_terminal}). Moreover, the same real-time control was synchronized with an existing android joystick app (section \ref{sec::B32_app}), which allowed us to control Heicub via a smartphone for the first time. Following that, a literature review revealed that no efforts were performed in utilizing neural networks for autonomous navigation of humanoid robots. To compensate for this shortcoming, it was investigated whether behavioral cloning and reinforcement learning were possible candidates to train neural networks on autonomous navigation, which are described in the following two sections.\\\\
The successfully implemented pattern generation library led to the aim of completely replacing the human user from the control loop of figure \ref{fig::2_cl}. Since no effort was being made in utilizing neural networks as substitutes, this work focused on replacing the human user by neural networks and, therefore, behavioral cloning was depicted as a possible candidate to train a neural network on autonomous navigation. Preliminary aims had to be achieved, so to utilize depth information as input to the neural network, and since Heicub does not support RGBD cameras, but only stereo RGB cameras (section \ref{fig::3_hei}), depth maps had to be extracted via existing algorithms from OpenCV \cite{opencv_library}. As such, Heicub's cameras got calibrated (figure \ref{fig::421_rect}) to allow for image rectification, which is a requirement for depth map extraction from stereo cameras. The obtained camera's intrinsic and extrinsic parameters are listed in table \ref{tab::421_extrinsics}. It was then proven that simple block matching algorithms were not sufficient to allow for a depth map extraction (figure \ref{fig::422_disp} (a)), but instead it was shown how confidence weighted least squares disparity maps solved the issue (figure \ref{fig::422_sigma_lambda}). For the behavioral cloning, we decided to try and train Heicub on finding a fire extinguisher within a room. The dataset, which was acquired for this task, comprises a total of $2\times134401$ images, which make up for $4.7\,\text{GB}$ (figure \ref{fig::423_dataset}). A newly developed U-Net-Fully-Connected-LSTM network demonstrated the best results on the acquired images (figure \ref{fig::423_unet}). For comparative reasons, we decided to let Heicub perform benchmarking tests, which were designed to let a human user, as well as the trained neural network, find a fire extinguisher in a previously unseen environment. As benchmarking tasks, we considered straight walking (figures \ref{fig::412_uc_straight} and \ref{fig::424_aw_basic_straight}), curved walking (figures \ref{fig::412_uc_curved} and \ref{fig::424_aw_basic_curved}), obstacle avoidance (figures \ref{fig::412_uc_obstacle} and \ref{fig::424_aw_basic_obstacle}), as well as searching for the fire extinguisher (figures \ref{fig::412_uc_sight} and \ref{fig::424_aw_basic_sight}). It could be demonstrated that the uncertainty within the neural network's decisions did not influence the robot's balance (figure \ref{fig::424_entropy_balance}), for which we evaluated the command signal's entropy against the zero moment point deviation from the ideal case of a linear inverted pendulum. Additional tests further showed how the U-Net-Fully-Connected-LSTM was able to interact safely with humans in a dynamic environment and how it learned to understand semantic features (figure \ref{fig::424_aw_additional_dynamic}). The second approach to train a neural network then was to utilize reinforcement learning, which is presented in the following section.\\\\
As it has recently shown good results for reinforcement learning tasks in general, proximal policy optimization \cite{schulman2017proximal} was chosen to train a neural network on autonomous navigation. The well-known algorithm got, therefore, implemented in C++ with PyTorch \cite{paszke2017automatic}, which is, to the best knowledge of the authors, the first implementation of its kind. The algorithm was first shown to converge for simple navigation tasks in a simulated test environment (figure \ref{fig::431_ppo_test}), and in contrast to the behavioral cloning approach, it does not rely on images as input, but instead on positional information. Due to temporal constraints, it was further induced that proximal policy optimization was not directly applicable to the real robot. It was therefore shown how proximal policy optimization could be combined with the nonlinear model predictive control to achieve autonomous navigation for humanoid robots in the same test environment (figure \ref{fig::432_nmpc_ppo_env}), which strongly benefited from the speed-up of the presented nonlinear model predictive control implementation (section \ref{sec::411_bm}). The fact that the algorithm had to be trained in simulation, rather than on the real robot, directly leads to some of the implications within this work that are presented in the following section.
\section{Implications and Limitations}
The implications of this work are very versatile, so are the limitations, and they can mainly be drawn with respect to the pattern generation library, as well as the two approaches for utilizing neural networks on autonomous navigation.\\\\
The implemented pattern generation library was shown to be fast enough to be used in a reinforcement learning setup. It, therefore, enables the future reader to explore further, and combine reinforcement learning approaches with optimal control. Limitations to the pattern generation library are the theoretically originating limits of the used sequential quadratic programming method, which is built upon the assumption that the current robot's state $\bm{x}_k$ is close to a solution (section \ref{sec::22_sqp}). Divergence of the solution may, therefore, be introduced by the center of mass feedback, which deviates from the linear inverted pendulum. An additional limitation of the implemented nonlinear model predictive control is that it is based on an analytic computation of the zero moment point, which is based on the assumption of a linear inverted pendulum, which the robot clearly does not represent.\\\\
It was shown that neural networks could be trained via behavioral cloning to solve navigation tasks. The presented behavioral cloning is general enough, such that it could easily be applied to tasks like corridor navigation, which were previously solved by complex models in \cite{faragasso2013vision}. Other multi-behavior state of the art approaches, as presented in the introductory section \ref{sec::1_in}, which are based on fuzzy logic, and that are used on top of existing navigation strategies, may completely be replaced by the presented method, since it combines multiple behaviors and the autonomous navigation into one solution. The great advantage of behavioral cloning over simultaneous localization and mapping approaches is that it works for dynamic environments. Behavioral cloning is further not dependent on a reference coordinate system, and is therefore also not prone to slipping, which is a problem in humanoid robotics applications. The presented method is fast, and it was shown in section \ref{sec::423_da} that it takes $4\,\text{ms}$ to perform inference with the used neural network. Given that it was run on a GPU, and that tensor processing units are currently under development, which already outperform GPUs and CPUs by $15-30\times$ in terms of energy efficiency and speed \cite{jouppi2017datacenter}, it becomes clear how similar methods will become increasingly important for energy-critical applications like humanoid robots. Although we could solve the task of finding the fire-extinguisher for three of the four benchmarking cases, the robot did not manage to avoid the obstacle, and go to the fire extinguisher at the same time. It only managed to avoid the obstacle, and then lost sight of the fire extinguisher. Other limitations are that a broader spacial understanding of the environment is yet missing. The designed behavioral cloning network architecture, furthermore, had a limited temporal understanding, since it only used the five most recent images as input. An additional issue, just as for fuzzy logic based behaviors, is that the neural network performs decisions as a result of incomplete measures of the environment, which may in principle lead to undefined actions. This is, however, a limitation to all existing systems, but in contrast to them, behavioral cloning may also perform undefined decisions on encountering unseen environmental states, which can be compensated for by ensuring a versatile dataset to train from.\\\\
Concerning the reinforcement learning part of this thesis, the implemented cutting-edge proximal policy optimization was the first of its kind that got implemented in C++. It was successfully shown how reinforcement learning could be used on top of optimal control to achieve autonomous navigation, which directly considers the system's physics. The implemented algorithm is, moreover, general enough to learn other tasks than autonomous navigation. Limitations are, however, that the presented simulation environment did not support obstacles yet, as it relied on the built-in obstacle avoidance of the nonlinear model predictive control. In contrast to the behavioral cloning approach, the reinforcement learning approach, as it got trained in simulation, does not work on images, but on the current location of the goal. It, therefore, requires an additional environment mapping algorithm to work properly, which directly leads to future work that is presented in the following section.
\section{Future Work}
This work opens many possibilities for future work. The implemented nonlinear model predictive control could be extended by replacing the linear inverted pendulum computation of the zero moment point by a neural network-based computation, for which data had to be acquired by running the implemented pattern generation, and by storing joint angles and their related zero moment points, from which a neural network could learn to map joint angles to the measured zero moment point.\\\\
The framework, which got developed in the course of this thesis, can be used to not only train humanoid robots on navigation, but on general behaviors, such as grasping objects. New network architectures can be found very quickly by prototyping with the presented fusion of Python with C++. The future reader should further pay attention to re-investigate on the used neural network's cost function, since the behavior may better be represented by a Kullback-Leibler divergence than by the used mean squared error. However, it is not straight forward to replace one error function by the other, and more research must be done. A network architecture should be developed that allows for a better spacial understanding of the scene. Additionally, a better temporal understanding could be achieved by introducing a longer sequence of past images to the developed U-Net-Fully-Connected-LSTM, or by utilizing attention-based neural networks instead of LSTMs, should memory be an issue. \\\\
While the proximal policy optimization turned out to be not suitable for reinforcement learning on the real robot, it showed to be applicable to a simulation environment, from where it can be easily carried to the real robot. However, further steps need to be taken, and a simultaneous localization and mapping algorithm must be utilized, as the neural network, which got trained via reinforcement learning, relies on spatial information. The simultaneous localization and mapping algorithm will, however, directly benefit from the camera calibration, which got carried out within the scope of this thesis. The implemented proximal policy optimization algorithm is, moreover, general enough to be applied to other tasks. In combination with the very fast nonlinear model predictive control it can, for example, be used in order to investigate learning optimal control solutions in a reinforced setting.