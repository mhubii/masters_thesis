\FloatBarrier
\section{Deep Learning Integration}
\label{sec::33_dl}
Again, the first step within the deep learning pipeline is the image processing, as already introduced in section  \ref{sec::33_ip}. The camera calibration within this work was done using OpenCV \cite{opencv_library}. Exemplary code with which we did the depth map parameter tuning in section \ref{sec::422_dp}, can be found at the provided \href{https://github.com/mhubii/nmpc_pattern_generator/blob/master/src/tune_disp_map.cpp}{\underline{link}}. It relies on the rectification matrices $\bm{R}_i$, and the projection matrices $\bm{P}_i$ that got explained in section \ref{sec::33_ip}. YAML files store them inside of the io\_module folder of figure \ref{fig::31_folder}. They are highly dependent on the robot but will not change for Heicub over time, for why the future reader will be able to reuse them. As already explained in section \ref{sec::51_bc}, Heicub's recorded images and the corresponding velocities got stored inside of a folder. Locations to the image files, as well as the velocities, were then stored in a text file. For a faster prototyping, we then decided to implement the deep learning training pipeline within Python with PyTorch \cite{paszke2017automatic}, for which the routine can be found at the provided \href{https://github.com/mhubii/nmpc_pattern_generator/blob/master/libs/learning/python/train_rgbd.py}{\underline{link}}. The trained model got then converted to a just in time (JIT) script that loads into the executables in the src folder of figure \ref{fig::31_folder} via
\begin{minted}{cpp}
  auto module = torch::jit::load(net_location);
\end{minted}
The code to convert a model, which got trained in Python, to C++, is part of the python folder and can be found at the following \href{https://github.com/mhubii/nmpc_pattern_generator/blob/master/libs/learning/python/python_to_cpp.py}{\underline{link}}. For Heicub, there was an additional step to be done, which is explained in section \ref{sec::6_co}, but given the camera images and depth maps are provided as \inlinecode{C++}{cv::Mat}, it is possible to convert them to a \inlinecode{C++}{torch::Tensor}, which can then be concatenated into a single RGBD tensor. We temporarily saved a sequence of these tensors, which enabled us to forward them through our long short-term memory based neural network, for which the architecture is given in figure \ref{fig::423_unet}. For the behavioral augmentation, this can, for example, be seen at the provided \href{https://github.com/mhubii/nmpc_pattern_generator/blob/719fde0bb73925923de85cbf379c5523e075dfeb/src/behavioural_augmentation_real_robot_external_data.cpp#L625}{\underline{link}}. Once the neural network predicts a desired velocity for the pattern generation, one only has to convert the desired velocity back to an \inlinecode{C++}{Eigen::Vector3d}, which is the datatype that the pattern generation library works with. This velocity is then forwarded, just as described in section \ref{sec::32_pg}, via the \inlinecode{}{NMPCPatternGenerator::SetVelocityReference} method, and a new iteration is executed. In the following section, we will explain how the developed pipelines can be used together with Heicub. It is therefore mainly required to implement the control loop of figure \ref{fig::2_cl}. 