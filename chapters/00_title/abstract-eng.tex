In this work, the capability of neural networks for autonomous navigation of humanoid robots is investigated. Therefore, a nonlinear model predictive control that allows for real-time generation of balanced walking trajectories is implemented and evaluated. Following that, the neural networks are trained on autonomous navigation in two different ways, namely via behavioral cloning, and via reinforcement learning. The first of which relies on stereo RGB images as input, and it is demonstrated that the introduction of depth maps to the vision pipeline is sufficient for the learning of versatile motion strategies in various dynamic and static environments. This simplicity is identified as a well-suited addition to the avoidance of convex obstacles, which are represented by constraints to the solution of the implemented nonlinear model predictive control. The reinforcement learning approach is then shown to be trainable in a simulation environment, and the emergence of rich self-taught policies is observed, which are equally applicable to the real robot. All of the experiments are carried out on Heicub, a descendant of the iCub, which was especially designed for optimal control in locomotion at the Istituto Italiano di Tecnologia in Genova. The evaluation of balance criteria finally reveals that there is no superiority of a human controller over an artificial agent. 