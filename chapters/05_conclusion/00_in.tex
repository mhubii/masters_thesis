\label{sec::5_co}
This work began as a spin-off from previous work, which has been carried out at our group \cite{stein2017closed}. It proposed to re-implement the nonlinear model predictive controller together with the inverse kinematics into an enclosed library, in order to compensate for delays that were induced by the YARP network, which had to manage these two building blocks separately. The presented implementation ran successfully within a single thread of the YARP network (see figure \ref{fig::341_yarp}), and furthermore achieved a speedup of $600\,\%$ (section \ref{sec::411_bm}), when compared to previous implementations. For there was no new research that could have been done, we focused on the second claim of the foregoing work, which indicated the use of additional constraints for the inverse kinematics that could have possibly enabled Heicub to walk without hip dislocations. By introducing additional rotational constraints to the hip, we were able to show how arbitrary trajectories could be unlocked for Heicub (section \ref{sec::412_pt}). This finding then successfully led to a flexible real-time control with the implemented terminal user interface and the android joystick app (sections \ref{sec::B31_terminal} and \ref{sec::B32_app}). Following that, a literature review revealed that no efforts were performed in utilizing neural networks for autonomous navigation of humanoid robots. To compensate for this shortcoming, we investigated behavioral cloning and proximal policy optimization as possible candidates to train neural networks on the task. Preliminary aims had to be achieved, so to utilize depth information as input to the neural networks. Among them, image processing steps were used to supplement Heicub with depth vision, which does not support RGBD cameras, but only stereo RGB cameras (section \ref{fig::34_hei}). As such, Heicub's cameras got calibrated (figure \ref{fig::421_rect}) to allow for image rectification, which is a requirement for depth map extraction from stereo cameras. The obtained camera's intrinsic and extrinsic parameters are listed in table \ref{tab::421_extrinsics}. It was then proven that simple block matching algorithms were not sufficient to allow for a depth map extraction (figure \ref{fig::422_disp} (a)), but instead it was shown how confidence weighted least squares disparity maps solved the issue (figure \ref{fig::422_sigma_lambda}). For the behavioral cloning, we decided to try and train Heicub on finding a fire extinguisher within a room. The dataset, which was acquired for this task comprises a total of $2\times134401$ images, which make up for $4.7\,\text{GB}$ (figure \ref{fig::423_dataset}). A newly developed U-Net-Fully-Connected-LSTM network demonstrated the best results on the images (figure \ref{fig::423_unet}). For comparative reasons, we have then decided to let Heicub perform benchmarking tests, which were designed to let a human user, as well as the trained neural network, find a fire extinguisher in a previously unseen environment. As benchmarking tasks, we considered straight walking (figures \ref{fig::412_uc_straight} and \ref{fig::424_aw_basic_straight}), curved walking (figures \ref{fig::412_uc_curved} and \ref{fig::424_aw_basic_curved}), obstacle avoidance (figures \ref{fig::412_uc_obstacle} and \ref{fig::424_aw_basic_obstacle}), as well as searching for the fire extinguisher (figures \ref{fig::412_uc_sight} and \ref{fig::424_aw_basic_sight}). It could successfully be demonstrated that the uncertainty within the neural network's decisions did not influence the robot's balance (figure \ref{fig::424_entropy_balance}), for which we evaluated the command signal's entropy against the zero moment point deviation from the ideal case of a linear inverted pendulum. Additional tests could further show how the U-Net was able to interact with humans in a dynamic environment and how it learned to understand semantic features (figure \ref{fig::424_aw_additional_dynamic}). Given the promising results with behavioral cloning, we went on to evaluate the implemented proximal policy optimization. The algorithm was shown to converge for simple navigation tasks in a simulated test environment (figure \ref{fig::425_ppo_test}).
\\\\
The implications of this work are very versatile. It was shown that neural networks could be trained via behavioral cloning to solve navigation tasks. The presented formulation is general enough, such that it could easily be applied to tasks like corridor navigation, which were previously solved by complex models in \cite{faragasso2013vision}. Other multi-behavior state of the art approaches, which are based on fuzzy logic, and that are used on top of existing navigation strategies, may completely be replaced by similar methods as the presented one, which combines multiple behaviors and the navigation into one solution. The presented method is fast, and it was shown in section \ref{sec::423_da} that it takes $4\,\text{ms}$ to perform inference with the used neural network. Given that it was run on a GPU, and that tensor processing units are currently under development, which already outperform GPUs and CPUs by $15-30\times$ in terms of energy efficiency and speed \cite{jouppi2017datacenter}, it becomes clear how similar methods will become increasingly important for energy-critical applications like humanoid robots. Concerning the reinforcement learning part of this thesis, the implemented cutting-edge proximal policy optimization algorithm has gained quite some attraction on GitHub from people all around the world, including the head developers of PyTorch.
\\\\
Limitations to this work are the theoretically originating limits of the used sequential quadratic programming method, which is built upon the assumption that the current robot's state $\bm{x}_k$ is close to a solution (section \ref{sec::212_sqp}). Divergence of the solution may be introduced by feedback, which deviates from the optimal behavior. The designed network architecture, although it may, in principle, take more images as input, had a limited temporal understanding, since we only used the five most recent images as input. We, therefore, propose future work that could be done in the following paragraph.
\\\\
This work opens many possibilities for future work. The framework, which got developed in the course of this thesis, can be used to not only train humanoid robots on navigation, but on general behaviors as well. New network architectures can be found very quickly by prototyping with the presented Python and C++ combination. The future reader should further pay attention to re-investigate on the used neural network's cost function, since the behavior may better be represented by a Kullback-Leibler divergence than by the used mean squared error. However, it is not straight forward to replace one error function by the other, and more research must be done. Although we could solve the task of finding the fire-extinguisher for three of the four benchmarking cases, the robot did not manage to avoid the obstacle, and go to the fire extinguisher at the same time. It only managed to avoid the obstacle, and then lost sight of the fire extinguisher. This could be solved by introducing a longer sequence of past images, or by utilizing attention-based neural networks instead of LSTM, should memory be an issue. While the proximal policy optimization turned out to be not suitable for reinforcement learning on the real robot, it can be used to combine it with the presented nonlinear model predictive control, in order to investigate on learning optimal control solutions.