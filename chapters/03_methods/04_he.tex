\section{Heicub}
\label{sec::34_he}
Heicub is a descendant of the iCub, which was specially designed for optimal control in locomotion at the Istituto Italiano di Tecnologia in Genova. It is used within this thesis to test the implemented algorithms, for which it provides all the required actuators and sensors, which are shortly introduced in the following section. 
\subsection{Heicub's Sensors}
\label{sec::341_hs}
As shown in figure \ref{fig::34_hei} (c), Heicub has a total of 21 degrees of freedom, of which six correspond to the floating base, and 15 to the rotational joints. Its two RGB cameras have a resolution of $240\times320$ pixels at a framerate of $60\,\text{fps}$, and are located within the chest, as shown in figure \ref{fig::34_hei} (a). The force-torque sensors, with which we compute the zero moment point, are located within the ankles and the hip. According to equations \ref{eq::211_x_pos_zmp_simp} and \ref{eq::211_y_pos_zmp_simp}, we only require the force-torque sensors from the ankles, where $d=0.03\,\text{m}$. Heicub also supports skin sensors, and an inertial measurement unit, but both are not used within this thesis. The use of the robot, and all its sensors, is well described within the appendix, starting from section \ref{sec::B_su}. Furthermore, Heicub has a Gazebo simulation model, which is shown in figure \ref{fig::34_hei}, and for which the installation instruction can be found in section \ref{sec::A4_sm}. It provides the exact same functionality as its real equivalent does. This is achieved via Gazebo plugins, which communicate to the YARP network, just as Heicub does. The plugins can be installed by following the instructions in section \ref{sec::a52_plugins}. The Gazebo model offers a perfect environment for software development, as we can make sure that implemented algorithms work properly, prior to their usage on the real robot, which could take harm from wrongly designed software. YARP stands short for Yet Another Robot Platform, and its installation is explained in section \ref{sec::a52_yarp}. Most important for us is to understand the YARP network in more detail, as it allows us to communicate with the robot. It is therefore explained within the next section - Communication with Heicub.
\begin{figure}[h!]
	\centering
	\subcaptionbox{Heicub.}%
	[.3\linewidth]{\includegraphics[scale=.35]{chapters/03_methods/img/heicub.png}}
	\subcaptionbox{Gazebo model.}%
	[.3\linewidth]{\includegraphics[scale=.35]{chapters/03_methods/img/gazebo_heicub.png}}
	\subcaptionbox{Kinematic chain.}%
	[.3\linewidth]{\includegraphics[scale=.35]{chapters/03_methods/img/kinematic_tree.png}}
	\caption{Heicub, the robot, which we used for the evaluation of the implemented software. There exists the real robot (a), as well as a simulated version of it, which offer equivalent functionality (b). Heicub's degrees of freedom, which can be actuated, are shown in (c). All the joints are rotational joints.}
	\label{fig::34_hei}
\end{figure}
\FloatBarrier
\subsection{Communication with Heicub}
\label{sec::342_co}
With YARP \cite{metta2006yarp}, it is possible to directly interface the robot's motors, the cameras, and the force-torque sensors. Moreover, it enables the user to run multiple programs in parallel, which can then communicate with each other, which is of special importance for the control loop that was implemented within the scope of this thesis (see figure \ref{fig::341_yarp}).
\begin{figure}[h!]
	\hspace*{-1cm}
	\includegraphics[scale=.4]{chapters/03_methods/img/yarp_diag.png}
	\caption{YARP is used to run multiple threads in parallel, each of which is indicated by the dashed boxes. It further enables the individual threads to communicate with each other via ports, which exchange YARP objects. It enables communication to the real robot as well as to a simulated version of it. The diagram demonstrates the types of data, which are being used, and the functions that convert them. Notice that this is an extended version of figure \ref{fig::2_cl}.}
	\label{fig::341_yarp}
\end{figure}
As we know from the previous section, one of these threads is committed to make decisions with a neural network, given RGBD images. This thread is depicted as the artificial agent thread in figure \ref{fig::341_yarp}. The pattern generation thread performs the nonlinear model predictive control, given the velocity command of the artificial agent thread. Furthermore, there are two additional running threads that communicate with Heicub's motors. These threads are the read, and the write thread in figure \ref{fig::341_yarp}. The classes, which implement the communication to the robot, are located within the io\_module folder of figure \ref{fig::31_folder}. The \inlinecode{C++}{WriteJoints} class implements a \inlinecode{C++}{yarp::os::RateThread}, which is periodically being called, to accesses the motors, which are defined within the YAML configuration file, and changes the motors' settings to position direct mode. Therefore, whatever is being written to the port that \inlinecode{C++}{WriteJoints} uses to communicate with the YARP network, and which is defined in the YAML configuration file, directly gets executed on the robot's motors. This communication to Heicub's motors corresponds to the very lowest part of figure \ref{fig::341_yarp}, where, as extensively explain in section \ref{sec::32_pg}, the pattern generation uses the forward kinematics to generate joint angles $\bm{q}$, which are being written as \inlinecode{C++}{yarp::sig::Vector} to the \inlinecode{C++}{WriteJoints} rate thread. The reading of the robot's sensors is also implemented as part of the io\_module folder from figure \ref{fig::31_folder}. There are several classes, which implement \inlinecode{C++}{yarp::os::RateThread}s for different reading tasks. Among them are the \inlinecode{C++}{ReadJoints} class, which reads out the motor encoders to obtain the joint angles, the \inlinecode{C++}{ReadCameras} class, which reads out the cameras and pushes them as \inlinecode{C++}{yarp::sig::ImageOf<yarp::sig::PixelRgb>} onto the network (see figure \ref{fig::341_yarp}), as well as the \inlinecode{C++}{AppReader} class, and the \inlinecode{C++}{KeyReader} class, which handle the communication to the joystick app, and the terminal, respectively. Both, the \inlinecode{C++}{AppReader} class, and the \inlinecode{C++}{KeyReader} class, utilize NCurses to generate a user interface on the terminal, which is internally being trapped in a while loop until exit. They read out the input, which may originate from the joystick app, or the keyboard, and push them as the velocity commands onto the YARP network (see figure \ref{fig::341_yarp} left). The velocity commands, which are converted into an \inlinecode{C++}{Eigen::Vector3d} for the \inlinecode{C++}{NMPCGenerator::SetVelocityReference} method from section \ref{sec::32_pg}, may alternatively also originate from a neural network, which is being presented in figure \ref{fig::341_yarp}. The \inlinecode{C++}{GenerateVelocityCommands} rate thread, which enables this feature, is implemented as part of the src folder in \ref{fig::31_folder}, as it only utilizes the provided libraries. It can be found at the provided \href{https://github.com/mhubii/nmpc_pattern_generator/blob/719fde0bb73925923de85cbf379c5523e075dfeb/src/behavioural_augmentation_real_robot_external_data.cpp#L108}{\underline{link}}. Its main task is to read in the images, which are constantly being pushed to the YARP network by \inlinecode{C++}{ReadCameras}, and to convert them into \inlinecode{C++}{cv::Mat} matrices, for us to perform the image processing on them, which includes the rectification, and the depth map extraction that are explained in section \ref{sec::224_ip}. The \inlinecode{C++}{GenerateVelocityCommands} class additionally stores a sequence of the processed images in the form of a \inlinecode{C++}{std::vector<torch::Tensor>}. Whenever a new image is read from the YARP network, the oldest image within the \inlinecode{C++}{std::vector<torch::Tensor>} is being deleted, and all other images are shifted up by one index, such that the newest image is available as the first entry. This vector of tensors is then being converted into a single tensor, by concatenating the individual tensors along the first dimension, which is, by definition of the long short-term memory units, required in PyTorch. The concatenated tensor is further being converted into a \inlinecode{C++}{std::vector<torch::jit::IValue>}, such that the JIT script, which defines the neural network that got trained in Python (see \ref{sec::33_dl}), can forward it. The output is then obtained as a \inlinecode{C++}{torch::Tensor} yet again, which is being written in the form of a \inlinecode{C++}{yarp::sig::Vector} to the YARP network, such that,  as explained above, the pattern generation can use it as input. This pipeline works equivalently on the real robot, as well as on a simulated version of it in Gazebo \cite{koenig2004design}, for which install instructions are provided in the appendix \ref{sec::A4_sm}. Although it allowed us to prototype within the simulation, we will use the pipeline to train the real robot Heicub, as an example for our method, on finding a fire extinguisher within the next section - Experiments.